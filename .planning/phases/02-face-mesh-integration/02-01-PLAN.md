---
phase: 02-face-mesh-integration
plan: 01
type: execute
---

<objective>
Integrate MediaPipe Face Mesh to detect facial landmarks in real-time and visualize specific lip landmarks (13 and 14) on the video feed.

Purpose: Establish the facial landmark detection foundation required for mouth state analysis in Phase 3.
Output: Working face mesh detection with landmarks 13 and 14 highlighted on live video feed.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
./summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-face-mesh-integration/DISCOVERY.md
@.planning/phases/01-foundation-setup/01-01-SUMMARY.md
@main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Initialize MediaPipe Face Mesh</name>
  <files>main.py</files>
  <action>Add MediaPipe Face Mesh initialization at the top of main.py. Import mediapipe as mp, get mp.solutions.face_mesh. Create FaceMesh instance with parameters: static_image_mode=False (video stream mode for lower latency), max_num_faces=1 (single face per project requirement), refine_landmarks=False (base 468 landmarks sufficient), min_detection_confidence=0.5, min_tracking_confidence=0.5. Use context manager pattern (with statement) for proper resource cleanup. Do NOT use static_image_mode=True - that's for batch image processing and will hurt real-time performance.</action>
  <verify>Run script - no import errors, Face Mesh initializes without errors</verify>
  <done>MediaPipe imports work, FaceMesh instance created with correct video stream parameters</done>
</task>

<task type="auto">
  <name>Task 2: Integrate Face Mesh processing into OpenCV loop</name>
  <files>main.py</files>
  <action>Inside the main capture loop (after ret, frame = cap.read()), convert frame from BGR to RGB using cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) since OpenCV uses BGR but MediaPipe expects RGB. Process the RGB frame with face_mesh.process(). Check if results.multi_face_landmarks exists (can be None if no face detected) before accessing landmarks. Do NOT skip BGRâ†’RGB conversion - Face Mesh will produce incorrect results or fail with BGR input. Do NOT process the original BGR frame.</action>
  <verify>Run script - face detection runs without errors, no crashes when no face visible</verify>
  <done>Face Mesh processes each frame, handles no-face-detected case gracefully, RGB conversion in place</done>
</task>

<task type="auto">
  <name>Task 3: Draw landmarks 13 and 14 on video feed</name>
  <files>main.py</files>
  <action>When face_landmarks exist, extract landmarks 13 and 14 from face_landmarks.landmark[13] and face_landmarks.landmark[14]. Convert normalized coordinates to pixel coordinates: pixel_x = int(landmark.x * frame_width), pixel_y = int(landmark.y * frame_height). Draw circles at these positions using cv2.circle() with radius 5, color green (0, 255, 0), thickness -1 (filled). Add labels "13" and "14" next to each landmark using cv2.putText(). Do NOT forget to multiply by frame dimensions - normalized coordinates are in [0.0, 1.0] range and won't display correctly without conversion.</action>
  <verify>Run script and show face to camera - green circles appear at lip positions (landmarks 13 and 14), circles track face movement smoothly</verify>
  <done>Landmarks 13 and 14 visible as green circles with labels, tracking works in real-time, display updates without lag</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Script runs without errors or warnings
- [ ] Face detection activates when face is visible
- [ ] Landmarks 13 and 14 drawn accurately on lip positions
- [ ] Display updates smoothly in real-time (no significant lag)
- [ ] 'q' key still quits application properly
- [ ] No crashes when face moves out of frame
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- MediaPipe Face Mesh integrated successfully
- Landmarks 13 and 14 visualized on video feed
- Real-time performance maintained
- No errors when face not detected
  </success_criteria>

<output>
After completion, create `.planning/phases/02-face-mesh-integration/02-01-SUMMARY.md`:

# Phase 2 Plan 1: Face Mesh Integration Summary

**[Substantive one-liner - what shipped, not "phase complete"]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `main.py` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Phase 2 complete, ready for Phase 3 (Mouth Detection Logic).
</output>
