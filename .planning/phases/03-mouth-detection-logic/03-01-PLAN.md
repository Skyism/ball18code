---
phase: 03-mouth-detection-logic
plan: 01
type: execute
---

<objective>
Implement Euclidean distance calculation between lip landmarks 13 and 14, establish threshold-based mouth open/closed state detection, and provide clear real-time visual feedback on the video feed.

Purpose: Transform raw landmark coordinates into actionable mouth state detection with visual indicators for system validation and user feedback.
Output: Working mouth detection system displaying distance, state (open/closed), and color-coded visual feedback in real-time.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-face-mesh-integration/02-01-SUMMARY.md
@main.py

**Tech stack available:** opencv-python, mediapipe (v0.10.31 with FaceLandmarker API)

**Established patterns:**
- MediaPipe Face Landmarker with video mode processing
- BGR to RGB conversion for MediaPipe
- Normalized to pixel coordinate conversion
- Green circle visualization for landmarks

**Constraining decisions:**
- Phase 2: Use MediaPipe FaceLandmarker API (new v0.10.31) with video mode processing and timestamp tracking
- Phase 1: opencv-python without contrib modules for lean deployment

**Current state:**
Landmarks 13 and 14 are tracked in real-time and visualized with green circles and labels. Ready for distance calculation and state detection logic.
</context>

<tasks>

<task type="auto">
  <name>Task 1: Calculate and display Euclidean distance between landmarks 13 and 14</name>
  <files>main.py</files>
  <action>Add distance calculation after landmark pixel coordinate conversion. Use standard Euclidean distance formula: sqrt((x2-x1)² + (y2-y1)²). Calculate distance in pixels between landmarks 13 and 14. Display distance value on frame using cv2.putText() in top-left corner (coordinates 10, 30) with white text, font FONT_HERSHEY_SIMPLEX, scale 0.7, thickness 2. Format as "Distance: XXX px" rounded to 0 decimal places. Do NOT use normalized coordinates for distance - they won't represent actual mouth opening accurately. Calculate distance AFTER converting to pixel coordinates.</action>
  <verify>Run script, move face closer/farther from camera - distance value updates in real-time and changes when mouth opens/closes</verify>
  <done>Distance calculation works, value displays in top-left corner, updates smoothly, increases when mouth opens</done>
</task>

<task type="auto">
  <name>Task 2: Implement threshold-based mouth state detection</name>
  <files>main.py</files>
  <action>Define MOUTH_OPEN_THRESHOLD constant at top of file (start with value 20 pixels as baseline - this will be tunable). After distance calculation, compare distance to threshold: if distance > MOUTH_OPEN_THRESHOLD, mouth is "OPEN", else "CLOSED". Store state in variable mouth_state. The threshold represents minimum vertical distance in pixels between upper and lower lip landmarks to consider mouth as open. Do NOT use percentage-based thresholds - absolute pixel distance is more reliable across different face sizes in frame. Threshold of 20 pixels works for typical webcam distance, but note this may need adjustment based on camera resolution and distance.</action>
  <verify>Run script - mouth_state variable correctly reflects "OPEN" when mouth opens (distance > 20), "CLOSED" when mouth closes (distance <= 20)</verify>
  <done>State detection logic works, threshold comparison accurate, state transitions smoothly between open and closed</done>
</task>

<task type="auto">
  <name>Task 3: Add color-coded visual feedback for mouth state</name>
  <files>main.py</files>
  <action>Display mouth state text below distance (coordinates 10, 60) using cv2.putText(). Text format: "State: [OPEN/CLOSED]". Color-code based on state: green (0, 255, 0) for CLOSED, red (0, 0, 255) for OPEN. Use same font (FONT_HERSHEY_SIMPLEX), scale 0.7, thickness 2. Additionally, change the landmark circles' color to match state - currently green (0, 255, 0), update to use state color (green for closed, red for open). This provides dual visual feedback: text label + colored landmark circles. Do NOT add background rectangles or boxes around text - keep display clean and minimal.</action>
  <verify>Run script - state text displays below distance, color changes red when mouth opens, green when closed, landmark circles also change color to match state</verify>
  <done>Visual feedback works, state text visible, colors update correctly (red=open, green=closed), landmark circles sync with state color, no display glitches</done>
</task>

</tasks>

<verification>
Before declaring phase complete:
- [ ] Script runs without errors
- [ ] Distance calculation accurate and updates in real-time
- [ ] Mouth state detection responds to actual mouth opening/closing
- [ ] Visual feedback (text + colored circles) clearly indicates current state
- [ ] State transitions smooth without flickering
- [ ] Display updates maintain real-time performance (no lag)
- [ ] 'q' key still quits application
</verification>

<success_criteria>

- All tasks completed
- All verification checks pass
- Euclidean distance calculation implemented and displayed
- Threshold-based state detection working accurately
- Color-coded visual feedback (red=open, green=closed) functioning
- Real-time performance maintained
- Ready for Phase 4 (Raspberry Pi optimization)
</success_criteria>

<output>
After completion, create `.planning/phases/03-mouth-detection-logic/03-01-SUMMARY.md`:

# Phase 3 Plan 1: Mouth Detection Logic Summary

**[Substantive one-liner - what shipped, not "phase complete"]**

## Accomplishments

- [Key outcome 1]
- [Key outcome 2]

## Files Created/Modified

- `main.py` - Description

## Decisions Made

[Key decisions and rationale, or "None"]

## Issues Encountered

[Problems and resolutions, or "None"]

## Next Phase Readiness

Phase 3 complete, ready for Phase 4 (Raspberry Pi Optimization). Mouth state detection working with tunable threshold.
</output>
